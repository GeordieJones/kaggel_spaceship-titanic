{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# set up for further filtering\n",
    "train_df['NameLength'] = train_df['Name'].str.len()\n",
    "train_df['Cabin'] = train_df['Cabin'].astype(str)\n",
    "train_df['Cabin'] = train_df['Cabin'].str[0] + train_df['Cabin'].str[-1]\n",
    "train_df['total spending'] = train_df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "train_df['VIP'] = train_df['VIP'].fillna(False).astype(int)\n",
    "train_df['CryoSleep'] = train_df['CryoSleep'].fillna(False).astype(int)\n",
    "\n",
    "# needed to calculate the distribution of positives to sort catagorical features\n",
    "transported_true = train_df[train_df['Transported'] == True]\n",
    "\n",
    "# sort homeplanet catagories by True values percentages\n",
    "counts = train_df['HomePlanet'].value_counts()\n",
    "transported_counts = transported_true.groupby('HomePlanet').size()\n",
    "percentages = transported_counts/counts\n",
    "homeplanet_map = percentages.sort_values(ascending=False)\n",
    "planet_map = {planet: rank for rank, planet in enumerate(homeplanet_map.index)}\n",
    "train_df['HomePlanet'] = train_df['HomePlanet'].map(planet_map)\n",
    "\n",
    "\n",
    "# sort destination catagories by True values percentages\n",
    "Destcounts = train_df['Destination'].value_counts()\n",
    "transDest_counts = transported_true.groupby('Destination').size()\n",
    "percent_vals = transDest_counts/Destcounts\n",
    "dest_map = percent_vals.sort_values(ascending=False)\n",
    "d_map = {planet: rank for rank, planet in enumerate(dest_map.index)}\n",
    "train_df['Destination'] = train_df['Destination'].map(d_map)\n",
    "\n",
    "# sort Cabin catagories by True values percentages\n",
    "# This does not need to be divided by counts as in my previous notebook it is \n",
    "# shown that there is less than a 1% differance in size of groups\n",
    "cabin_counts = train_df[train_df['Transported'] == True].groupby('Cabin').size()\n",
    "sorted_cabins = cabin_counts.sort_values(ascending=False).index\n",
    "cabin_mapping = {cabin: rank for rank, cabin in enumerate(sorted_cabins)}\n",
    "train_df['Cabin_ordinal'] = train_df['Cabin'].map(cabin_mapping).fillna(-1).astype(int)\n",
    "\n",
    "\n",
    "# Targets\n",
    "targets = train_df['Transported']\n",
    "\n",
    "\n",
    "#drop unused Catagories\n",
    "filtered_X = train_df[['HomePlanet', 'CryoSleep', 'Age','VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','NameLength', 'total spending', 'Cabin_ordinal']]\n",
    "filtered_X = filtered_X.fillna(filtered_X.median())\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_X, targets, test_size=0.15, stratify=targets, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, clone\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "pipe_knn = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    ")\n",
    "pipe_nb = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    GaussianNB()\n",
    ")\n",
    "pipe_mlp = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=20)\n",
    ")\n",
    "\n",
    "pipe_ext = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ExtraTreesClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=3,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=20,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define your base pipelines\n",
    "pipe_rf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(\n",
    "        random_state=20,\n",
    "        n_estimators=400,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=3,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "#random_state = 20,max_depth = 5,reg_lambda = 10,subsample=1.0,min_child_weight = 5,learning_rate = 0.2,n_estimators =100,reg_alpha=0.1, colsample_bytree = 0.8\n",
    "pipe_xgb = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    XGBClassifier(\n",
    "        random_state=20,\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        min_child_weight = 5,\n",
    "        reg_lambda = 10,\n",
    "        learning_rate=0.02,\n",
    "        subsample=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=20, max_iter=500)\n",
    ")\n",
    "\n",
    "pipe_svc = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(probability=True, random_state=20)\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": pipe_rf,\n",
    "    \"XGB\": pipe_xgb,\n",
    "    \"LogisticRegression\": pipe_lr,\n",
    "    \"SVC\": pipe_svc,\n",
    "    \"KNN\": pipe_knn,\n",
    "    \"NaiveBayes\": pipe_nb,\n",
    "    \"MLP\": pipe_mlp,\n",
    "    \"ExtraTrees\": pipe_ext,\n",
    "}\n",
    "\n",
    "\n",
    "# Store CV results for each model\n",
    "cv_results = {name: {\"scores\": [], \"models\": []} for name in models}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=15, shuffle=True, random_state=20)\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(X_train, y_train)):\n",
    "        model = clone(pipeline)\n",
    "        model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "        score = model.score(X_train.iloc[test_idx], y_train.iloc[test_idx])\n",
    "        \n",
    "        cv_results[name][\"scores\"].append(score)\n",
    "        cv_results[name][\"models\"].append(model)\n",
    "        \n",
    "    mean_acc = np.mean(cv_results[name][\"scores\"])\n",
    "    std_acc = np.std(cv_results[name][\"scores\"])\n",
    "    print(f\"{name}: {mean_acc:.3f} +/- {std_acc:.3f}\")\n",
    "\n",
    "# ---- Voting classifier on final retrained models ----\n",
    "# Usually, retrain each on full dataset for final voting model\n",
    "final_rf   = clone(pipe_rf).fit(X_train, y_train)\n",
    "final_xgb  = clone(pipe_xgb).fit(X_train, y_train)\n",
    "final_lr   = clone(pipe_lr).fit(X_train, y_train)\n",
    "final_svc  = clone(pipe_svc).fit(X_train, y_train)\n",
    "final_knn  = clone(pipe_knn).fit(X_train, y_train)\n",
    "final_nb   = clone(pipe_nb).fit(X_train, y_train)\n",
    "final_mlp  = clone(pipe_mlp).fit(X_train, y_train)\n",
    "final_ext  = clone(pipe_ext).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "estimatormodels = [\n",
    "    ('rf', final_rf),\n",
    "    ('xgb', final_xgb),\n",
    "    ('lr', final_lr),\n",
    "    ('svc', final_svc),\n",
    "    ('knn', final_knn),\n",
    "    ('nb', final_nb),\n",
    "    ('mlp', final_mlp),\n",
    "    ('ext', final_ext)\n",
    "]\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=estimatormodels,\n",
    "    voting='soft'  # soft voting uses predicted probabilities\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "print(\"Voting classifier accuracy:\", voting_clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# High recall (favors True)\n",
    "rf_high_recall = RandomForestClassifier(\n",
    "    class_weight={0: 1, 1: 2},  # more weight on True class\n",
    "    random_state=20,\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# High precision (favors False)\n",
    "rf_high_precision = RandomForestClassifier(\n",
    "    class_weight={0: 2, 1: 1},  # more weight on False class\n",
    "    random_state=21,\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Neutral\n",
    "rf_neutral = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=22,\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit biased models\n",
    "rf_high_recall.fit(X_train, y_train)\n",
    "rf_high_precision.fit(X_train, y_train)\n",
    "rf_neutral.fit(X_train, y_train)\n",
    "\n",
    "pred_recall = rf_high_recall.predict(X_test)\n",
    "pred_prec = rf_high_precision.predict(X_test)\n",
    "pred_neut = rf_neutral.predict(X_test)\n",
    "\n",
    "print(f\"High Recall RF accuracy: {accuracy_score(y_test, pred_recall):.4f}\")\n",
    "print(f\"High Precision RF accuracy: {accuracy_score(y_test, pred_prec):.4f}\")\n",
    "print(f\"Neutral RF accuracy: {accuracy_score(y_test, pred_neut):.4f}\")\n",
    "\n",
    "# Get probability predictions for training meta-model\n",
    "proba_recall = rf_high_recall.predict_proba(X_train)[:, 1]\n",
    "proba_prec   = rf_high_precision.predict_proba(X_train)[:, 1]\n",
    "proba_neut   = rf_neutral.predict_proba(X_train)[:, 1]\n",
    "\n",
    "meta_X = np.column_stack([proba_recall, proba_prec, proba_neut])\n",
    "\n",
    "# Fit meta-model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(meta_X, y_train)\n",
    "\n",
    "proba_recall = rf_high_recall.predict_proba(X_test)[:, 1]\n",
    "proba_prec   = rf_high_precision.predict_proba(X_test)[:, 1]\n",
    "proba_neut   = rf_neutral.predict_proba(X_test)[:, 1]\n",
    "\n",
    "meta_X_test = np.column_stack([proba_recall, proba_prec, proba_neut])\n",
    "final_preds = meta_model.predict(meta_X_test)\n",
    "\n",
    "# Print meta-model accuracy\n",
    "print(f\"Meta-model stacking accuracy: {accuracy_score(y_test, final_preds):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Feature engineering on test data — apply same steps as train_df\n",
    "test_df['NameLength'] = test_df['Name'].str.len()\n",
    "test_df['Cabin'] = test_df['Cabin'].astype(str)\n",
    "test_df['Cabin'] = test_df['Cabin'].str[0] + test_df['Cabin'].str[-1]\n",
    "test_df['total spending'] = test_df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "test_df['VIP'] = test_df['VIP'].fillna(False).astype(int)\n",
    "test_df['CryoSleep'] = test_df['CryoSleep'].fillna(False).astype(int)\n",
    "\n",
    "test_df['HomePlanet'] = test_df['HomePlanet'].map(planet_map).fillna(-1).astype(int)\n",
    "test_df['Destination'] = test_df['Destination'].map(d_map).fillna(-1).astype(int)\n",
    "test_df['Cabin_ordinal'] = test_df['Cabin'].map(cabin_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Select features & fill NA\n",
    "filtered_test_X = test_df[['HomePlanet', 'CryoSleep', 'Age','VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','NameLength', 'total spending', 'Cabin_ordinal']]\n",
    "filtered_test_X = filtered_test_X.fillna(filtered_test_X.median())\n",
    "\n",
    "proba_recall = rf_high_recall.predict_proba(filtered_test_X)[:, 1]\n",
    "proba_prec = rf_high_precision.predict_proba(filtered_test_X)[:, 1]\n",
    "proba_neut = rf_neutral.predict_proba(filtered_test_X)[:, 1]\n",
    "\n",
    "meta_X_test = np.column_stack([proba_recall, proba_prec, proba_neut])\n",
    "\n",
    "# Final predictions\n",
    "final_preds = meta_model.predict(meta_X_test)\n",
    "\n",
    "# Build submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported': final_preds.astype(bool)\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('tug_war_model.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
